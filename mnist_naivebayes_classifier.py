# -*- coding: utf-8 -*-
"""MNIST_NaiveBayes_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cbaIYcdcWWh5X9hc44lWIm4JcqU2bgFo
"""

# Step 1: Load the MNIST Dataset

import numpy as np
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt

print("Loading MNIST dataset...")
mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False, parser='auto')

X = mnist.data # Features (pixel data of images)
y = mnist.target # Labels (the actual digit)

print(f"Dataset loaded. X shape: {X.shape}, y shape: {y.shape}")
print(f"First 5 labels: {y[:5]}")

# Display the first digit as an example
plt.imshow(X[0].reshape(28, 28), cmap='binary')
plt.title(f"Label: {y[0]}")
plt.axis('off')
plt.show()

# Step 2: Preprocess the Data (Splitting and Scaling)

from sklearn.model_selection import train_test_split

print("Splitting data into training and testing sets...")
# Split the data into training and testing sets
# We'll use 70% for training and 30% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

print("\nScaling pixel values from 0-255 to 0-1...")
# Scale pixel values to be between 0 and 1
# This helps the model perform better as all features are on a similar scale
X_train = X_train / 255.0
X_test = X_test / 255.0

print("Data preprocessing complete.")

# Step 3: Implement Gaussian Naive Bayes from Scratch

class GaussianNaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.parameters = {} # Stores mean and variance for each feature per class

        for c in self.classes:
            # Filter data for the current class
            X_c = X[y == c]

            # Calculate mean and variance for each feature for this class
            # Add a small epsilon to variance to prevent division by zero in prediction
            self.parameters[c] = {
                'mean': X_c.mean(axis=0),
                'variance': X_c.var(axis=0) + 1e-6 # Adding epsilon for stability
            }

    def _calculate_likelihood(self, x, mean, variance):
        # Gaussian probability density function (PDF)
        # P(x | class) = (1 / sqrt(2 * pi * variance)) * exp(-(x - mean)^2 / (2 * variance))
        exponent = -((x - mean)**2) / (2 * variance)
        likelihood = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(exponent)
        return likelihood

    def predict(self, X):
        predictions = [self._predict_single_sample(x) for x in X]
        return np.array(predictions)

    def _predict_single_sample(self, x):
        posteriors = []

        # Iterate through each class (digit)
        for c in self.classes:
            # P(class) - We assume equal prior probability for simplicity for now
            # For a more robust model, you would calculate this based on class frequency in training data
            prior = 1.0 / len(self.classes)

            # P(x_i | class) for each feature
            mean = self.parameters[c]['mean']
            variance = self.parameters[c]['variance']

            # Calculate likelihood for all features for this class
            # Using log-likelihood to avoid underflow with many small probabilities
            likelihoods = self._calculate_likelihood(x, mean, variance)

            # Sum of log-likelihoods + log(prior)
            # Add a small value to likelihoods to prevent log(0)
            posterior = np.sum(np.log(likelihoods + 1e-9)) + np.log(prior) # Add epsilon to avoid log(0)

            posteriors.append((posterior, c))

        # Return the class with the highest posterior probability
        return max(posteriors)[1]

print("GaussianNaiveBayes class defined.")

# Instantiate and train the model
print("Training the Gaussian Naive Bayes model...")
nb_classifier = GaussianNaiveBayes()
nb_classifier.fit(X_train, y_train)
print("Model training complete.")

# Make predictions on the test set
print("Making predictions on the test set...")
y_pred = nb_classifier.predict(X_test)
print("Predictions complete.")

# Evaluate the model (using accuracy)
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"\nTest Accuracy of our Naive Bayes Classifier: {accuracy:.4f}")

# Display a few predictions vs actual
fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for i, ax in enumerate(axes):
    idx = np.random.randint(0, len(X_test))
    ax.imshow(X_test[idx].reshape(28, 28), cmap='binary')
    ax.set_title(f"Pred: {y_pred[idx]}\nActual: {y_test[idx]}")
    ax.axis('off')
plt.tight_layout()
plt.show()

# Step 4: Save the Trained Model

import pickle

print("Saving the trained Naive Bayes model...")

# Define the filename for our model
model_filename = 'naive_bayes_mnist_model.pkl'

# Save ONLY the parameters and classes of the trained model
with open(model_filename, 'wb') as file:
    pickle.dump(nb_classifier.parameters, file)
    pickle.dump(nb_classifier.classes, file)

print(f"Model parameters and classes saved to '{model_filename}'")
print("\nYou can find this file in the file browser on the left sidebar of Colab (folder icon).")